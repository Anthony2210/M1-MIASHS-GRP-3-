---
title: "datascience knn"
output: html_document
date: "2025-09-17"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Choix du modeles knn

##1. Chargement des librairies

```{r cars}

library(dplyr)
library(ggplot2)
library(caret)
library(class)
library(corrplot)
library(pROC)

```


##2. Lecture des données


```{r}

farms_train <- read.csv("farms_train.csv", sep=",", dec=".", header=TRUE)
farms_test  <- read.csv("farms_test.csv",  sep=",", dec=".", header=TRUE)

# Aperçu rapide
dim(farms_train)
summary(farms_train)
table(farms_train$DIFF)

## 3. Statistiques descriptives et graphiques


ggplot(farms_train, aes(x=factor(DIFF))) +
  geom_bar(fill="blue") +
  labs(title="Répartition des exploitations (0 = saine, 1 = défaillante)",
       x="DIFF", y="Effectif") +
  theme_minimal()

cor_mat <- cor(farms_train[, -1])
corrplot(cor_mat, method="color", type="upper", tl.col="black")

## 4. Découpage apprentissage / validation

set.seed(123)
trainIndex <- createDataPartition(farms_train$DIFF, p=0.7, list=FALSE)
train_data <- farms_train[trainIndex, ]
valid_data <- farms_train[-trainIndex, ]

## 5. Normalisation min-max

normalize <- function(x) { (x - min(x)) / (max(x) - min(x)) }
train_scaled <- as.data.frame(lapply(train_data[, -1], normalize))
valid_scaled <- as.data.frame(lapply(valid_data[, -1], normalize))
test_scaled  <- as.data.frame(lapply(farms_test,  normalize))

train_scaled$DIFF <- factor(train_data$DIFF, levels=c(0,1), labels=c("saine","defaillante"))
valid_scaled$DIFF <- factor(valid_data$DIFF, levels=c(0,1), labels=c("saine","defaillante"))

x_train <- train_scaled[, -ncol(train_scaled)]
y_train <- train_scaled$DIFF
x_valid <- valid_scaled[, -ncol(valid_scaled)]
y_valid <- valid_scaled$DIFF

## 6. Recherche du meilleur K

maxK <- 50
txErreur <- numeric(maxK)

for (i in 1:maxK) {
  knn_pred <- knn(train=x_train, test=x_valid, cl=y_train, k=i)
  txErreur[i] <- mean(knn_pred != y_valid)*100
}

plot(txErreur, type="b", col="blue", pch=20,
     xlab="K (voisins)", ylab="Taux d'erreur (%)",
     main="Erreur vs Nombre de voisins")
abline(h=min(txErreur), col="red", lty=2)

best_k_loop <- which.min(txErreur)
cat(" Meilleur K trouvé automatiquement =", best_k_loop, 
    "avec erreur =", min(txErreur), "%\n")

## 7. Choix final de K = 5

k_fix <- 5
cat(" K =", k_fix, "\n")

knn_pred_best <- knn(train=x_train, test=x_valid, cl=y_train, k=k_fix)
confusionMatrix(knn_pred_best, y_valid)


# 8. Courbe ROC

knn_pred_prob <- attr(knn(train=x_train, test=x_valid, cl=y_train, k=k_fix, prob=TRUE), "prob")
knn_pred_class <- knn_pred_best
prob_defaillante <- ifelse(knn_pred_class=="defaillante", knn_pred_prob, 1-knn_pred_prob)

roc_obj <- roc(response = y_valid, predictor = prob_defaillante, levels=c("saine","defaillante"))
plot(roc_obj, col="blue", main="Courbe ROC - Validation", legacy.axes=TRUE)
auc(roc_obj)


# 9. Prédictions finales 

x_all <- train_scaled[, -ncol(train_scaled)]
y_all <- train_scaled$DIFF
x_test <- test_scaled

knn_final <- knn(train=x_all, test=x_test, cl=y_all, k=k_fix)

soumission <- data.frame(ID = 1:nrow(farms_test),
                         DIFF = ifelse(knn_final=="saine",0,1))
head(soumission)

write.csv(soumission, "soumission.csv", row.names=FALSE)

table(soumission$DIFF)
prop.table(table(soumission$DIFF))
```
